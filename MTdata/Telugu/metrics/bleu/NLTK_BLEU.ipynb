{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"XX79iDN_6xO3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682443575049,"user_tz":-330,"elapsed":25849,"user":{"displayName":"LALIT BISHT","userId":"16917600131278585489"}},"outputId":"edf8f6ca-4fc7-478c-e0e6-c2a9fec5c29b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VopY3sShdecN","executionInfo":{"status":"ok","timestamp":1682443577192,"user_tz":-330,"elapsed":2154,"user":{"displayName":"LALIT BISHT","userId":"16917600131278585489"}},"outputId":"478e8138-7b7e-4596-d4a9-911acb7429f9"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}],"source":["import nltk\n","nltk.download('punkt')"]},{"cell_type":"markdown","metadata":{"id":"3NWdwRgO_U4P"},"source":["**SENETENCE WISE BLEU**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1273,"status":"ok","timestamp":1682443645764,"user":{"displayName":"LALIT BISHT","userId":"16917600131278585489"},"user_tz":-330},"id":"KT3LaZkoxd9b","outputId":"9cc5ec0a-4a9e-461c-e6ba-7d3b6ae5d230"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n","The hypothesis contains 0 counts of 3-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n","/usr/local/lib/python3.9/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n","The hypothesis contains 0 counts of 4-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n","/usr/local/lib/python3.9/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n","The hypothesis contains 0 counts of 2-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n"]}],"source":["from nltk import word_tokenize\n","from nltk.translate.bleu_score import sentence_bleu\n","\n","translated_file = \"/content/drive/MyDrive/MTdata/Telugu/hypo.txt\"   # Telugu\n","reference_file=\"/content/drive/MyDrive/MTdata/Telugu/ref.txt\"\n","\n","f5 = open('/content/BLEU.txt', \"a+\")\n","with open(translated_file, \"r\") as f1, open(reference_file, \"r\") as f2:\n","  lines1 = [line.rstrip('\\n') for line in f1]\n","  #print(lines1)\n","  lines2 = [line.rstrip('\\n') for line in f2]\n","  #print(lines2)\n","  for l1,l2 in zip(lines1,lines2):\n","    tokens_ref=[word_tokenize(l2)]\n","    tokens_translate=word_tokenize(l1)\n","    score = sentence_bleu(tokens_ref, tokens_translate)\n","    f5.write(str(score)+'\\n')\n","f5.close()\n"]},{"cell_type":"markdown","metadata":{"id":"NpmT0w5vvHI8"},"source":["**CORPUS BLEU**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bOznfvx9nzC_"},"outputs":[],"source":["from sklearn.preprocessing import MinMaxScaler\n","import numpy as np\n","\n","scaler = MinMaxScaler(feature_range=(0,1))"]},{"cell_type":"code","source":["file = \"/content/drive/MyDrive/MTdata/Telugu/metrics/bleu/BLEU.txt\"\n","with open(file, 'r') as f:\n","  lines = [line.rstrip('\\n')[:8] for line in f]\n","  lst_int = [float(i) for i in lines]\n","  scaled = []\n","  for i in lst_int:\n","    val = (i - min(lst_int))/(max(lst_int)-min(lst_int))    # min-max scaling\n","    scaled.append(val)\n","    print(scaled)\n","  "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N1iRogDza_nF","executionInfo":{"status":"ok","timestamp":1682446379207,"user_tz":-330,"elapsed":10,"user":{"displayName":"LALIT BISHT","userId":"16917600131278585489"}},"outputId":"d8853f7d-d968-43e0-e902-6d1c4d391c5b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.7260624911648043, 0.00451723407115653, 0.23504706006894108, 0.03741003253949887, 0.3945138764261062, 0.11885565568356653, 0.9562051380412093, 1.0, 0.7440397470703953, 0.5803681892681353, 0.7789207216923572, 0.001719196529177847, 0.014702848413041864, 0.036064376728207614, 0.7423081027919978, 0.5202433957113499, 0.22192453460030495, 0.4058899285585788, 0.6378817697625339, 0.0009650794090631283, 0.6660487531793177, 0.5745358234315999, 0.04576918322632238, 0.4998402358447985, 0.0, 0.7476290284975524, 0.4963652572278669]\n"]}]},{"cell_type":"code","source":["file = \"/content/drive/MyDrive/MTdata/Telugu/metrics/bleu/expert_score.txt\"\n","with open(file, 'r') as f:\n","  lines = [line.rstrip('\\n') for line in f]\n","  lst_dble = [float(i) for i in lines]\n","  print((lst_dble))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CvMbYu8tit7S","executionInfo":{"status":"ok","timestamp":1682446383562,"user_tz":-330,"elapsed":627,"user":{"displayName":"LALIT BISHT","userId":"16917600131278585489"}},"outputId":"f90c5466-99fe-4837-97e9-aa9ff2808b44"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.4, 0.8, 0.5, 0.9, 0.5, 0.6, 0.8, 0.3, 0.4, 0.7, 0.6, 0.4, 0.7, 0.6, 0.7, 0.8, 0.6, 0.6, 0.7, 0.3, 0.7, 0.3, 0.5, 0.7, 0.8, 0.2, 0.6]\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","from scipy.stats import pearsonr\n","\n","corr, _ = pearsonr(scaled, lst_dble)\n","print('Pearsons correlation: %.3f' % corr)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mm-wzYygjDQD","executionInfo":{"status":"ok","timestamp":1682446416535,"user_tz":-330,"elapsed":571,"user":{"displayName":"LALIT BISHT","userId":"16917600131278585489"}},"outputId":"86602d2b-6b37-4ed0-b59c-40de8260e039"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Pearsons correlation: -0.194\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"ayf7D6JsQakS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"SN7Ktf6IQaVO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import spacy\n","nlp1 = spacy.load('/content/drive/MyDrive/hindi_lang/model-final')\n","\n","file1=\"/content/reference.txt\"\n","file2=\"/content/ref_pos\"\n","file3=\"/content/ref_dep\"\n","lines = [line.rstrip('\\n') for line in open(file1)]\n","\n","file_out=open(file2,\"w\")\n","file_out1=open(file3,\"w\")\n","for each_line in lines:\n","    doc = nlp1(each_line)\n","    #print(each_line)\n","    for token in doc:\n","        file_out.write(str(token.tag_))\n","        file_out.write(\" \")\n","        file_out1.write(str(token.dep_))\n","        file_out1.write(\" \")\n","    file_out.write(\"\\n\")\n","    file_out1.write(\"\\n\")\n","file_out.close()\n","file_out1.close()"],"metadata":{"id":"XZuRaKK-lEzP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Corpus BLEU"],"metadata":{"id":"d_xumKz0PcyZ"}},{"cell_type":"code","source":["import nltk\n","weights='0.25 0.25 0.25 0.25'\n","weight = [float(v) for v in weights.split()]\n","with open(\"/content/drive/MyDrive/PhD/projectfolder/LM/Outputs/kenlm/3L_2/en_hi/En_hn_kenlm_6\", 'r') as trans, open(\"/content/drive/MyDrive/PhD/projectfolder/LM/Outputs/hi_1000.txt\", 'r') as ref:\n","            tran_list, ref_list = [], []\n","            for tl in trans:\n","                rl = ref.readline()\n","                tran_list.append(tl.strip().split(' '))\n","                ref_list.append(rl.strip().split(' '))\n","print(nltk.translate.bleu_score.corpus_bleu(ref_list, tran_list, weight))"],"metadata":{"id":"y1ZYJxPoPV1m"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"11gxjCE6oAJT6r6KzP4H-vLZxoRhQyh6k","timestamp":1679242441578}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}